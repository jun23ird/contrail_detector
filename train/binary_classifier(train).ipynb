{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# from IPython.display import clear_output\n",
    "import os\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import keras.layers as KL\n",
    "# import keras.engine as KE\n",
    "import keras.models as KM\n",
    "from tensorflow.keras.initializers import GlorotNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "import wandb\n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/kaggle/contrail_detector/')\n",
    "import utils\n",
    "import models\n",
    "from configs import binary_classifier_config, Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata = pd.read_json(\"/content/drive/MyDrive/kaggle/contrail_detector/data/train_metadata.json\", dtype={'record_id': 'str'})\n",
    "valid_metadata = pd.read_json(\"/content/drive/MyDrive/kaggle/contrail_detector/data/valid_metadata.json\", dtype={'record_id': 'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your data augmentation parameters\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=90,  # Degree range for random rotations\n",
    "    width_shift_range=0.0,  # Fraction of total width for random horizontal shifts\n",
    "    height_shift_range=0.0,  # Fraction of total height for random vertical shifts\n",
    "    shear_range=0.0,  # Shear intensity\n",
    "    zoom_range=0.0,  # Random zoom range\n",
    "    horizontal_flip=True,  # Randomly flip images horizontally\n",
    "    fill_mode='nearest'  # Fill mode for pixels outside the boundaries\n",
    ")\n",
    "\n",
    "input_shape = (256,256,3)\n",
    "\n",
    "def load_data_train(record_id, path):\n",
    "    # make path objects\n",
    "    record_id = record_id.numpy().decode('utf-8')\n",
    "    path = path.numpy().decode('utf-8')\n",
    "    image_path = path + \"image/\" + str(record_id) + \".npy\"\n",
    "    # load the images for the corresponding paths\n",
    "    image = np.load(image_path)[..., Config.n_times_before, :].astype(np.float32) / 255.0\n",
    "    image = tf.image.resize(image, input_shape[:2])\n",
    "    label = train_metadata.loc[train_metadata.record_id == record_id, 'contrail_exists'].item()\n",
    "    return image, label\n",
    "\n",
    "def load_data_valid(record_id, path):\n",
    "    # make path objects\n",
    "    record_id = record_id.numpy().decode('utf-8')\n",
    "    path = path.numpy().decode('utf-8')\n",
    "    image_path = path + \"image/\" + str(record_id) + \".npy\"\n",
    "    # load the images for the corresponding paths\n",
    "    image = np.load(image_path)[..., Config.n_times_before, :].astype(np.float32) / 255.0\n",
    "    image = tf.image.resize(image, input_shape[:2])\n",
    "    label = valid_metadata.loc[valid_metadata.record_id == record_id, 'contrail_exists'].item()\n",
    "    return image, label\n",
    "\n",
    "def augment_data(image, label):\n",
    "    image = tf.py_function(func=lambda img: datagen.random_transform(img.numpy()), inp=[image], Tout=tf.float32)\n",
    "    return image, label\n",
    "\n",
    "id_to_data_train = lambda record_id, path: tf.py_function(func=load_data_train, inp=[record_id, path], Tout=(tf.float32, tf.float32))\n",
    "id_to_data_valid = lambda record_id, path : tf.py_function(func = load_data_valid, inp = [record_id, path], Tout = (tf.float32, tf.float32))\n",
    "augment_data_map = lambda image, label : tf.py_function(func = augment_data, inp=[image,label], Tout = (tf.float32,tf.float32))\n",
    "\n",
    "train_id = train_metadata.record_id.to_list()\n",
    "valid_id = valid_metadata.record_id.to_list()\n",
    "\n",
    "train_path = len(train_id)*[Path.train]\n",
    "valid_path = len(valid_id)*[Path.valid]\n",
    "\n",
    "train_id_dataset = tf.data.Dataset.from_tensor_slices(train_id)\n",
    "train_path_dataset = tf.data.Dataset.from_tensor_slices(train_path)\n",
    "\n",
    "valid_id_dataset = tf.data.Dataset.from_tensor_slices(valid_id)\n",
    "valid_path_dataset = tf.data.Dataset.from_tensor_slices(valid_path)\n",
    "\n",
    "train_dataset = tf.data.Dataset.zip((train_id_dataset, train_path_dataset)).map(id_to_data_train).map(augment_data_map).shuffle(Config.buffer_size).batch(Config.batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = tf.data.Dataset.zip((valid_id_dataset, valid_path_dataset)).map(id_to_data_valid).shuffle(Config.buffer_size).batch(Config.batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging and checkpoints\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs, labels, model, loss_fn, optimizer):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(inputs, training = True)\n",
    "    loss_value = loss_fn(predictions, labels)\n",
    "  gradients = tape.gradient(loss_value, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "  return loss_value\n",
    "\n",
    "def valid_step(inputs, labels, model, loss_fn, metric_fn):\n",
    "  predictions = model(inputs, training = False)\n",
    "  loss_value = loss_fn(predictions, labels)\n",
    "  metric_value = metric_fn(predictions, labels)\n",
    "  return loss_value, metric_value\n",
    "\n",
    "def train_model(train_dataset, valid_dataset, model, loss_fn, metric_fn, optimizer, epochs, checkpoint_directory, log_directory, max_to_keep=3):\n",
    "\n",
    "  # printout which device the model is on\n",
    "  print(model.layers[0].weights[0].device)\n",
    "\n",
    "  train_losses = []\n",
    "  valid_losses = []\n",
    "  valid_metrics = []\n",
    "\n",
    "  # Create a checkpoint object and checkpoint manager\n",
    "  checkpoint = tf.train.Checkpoint(step=tf.Variable(0),\n",
    "                                   optimizer=optimizer,\n",
    "                                   model=model)\n",
    "  manager = tf.train.CheckpointManager(checkpoint,\n",
    "                                       checkpoint_directory,\n",
    "                                       max_to_keep)\n",
    "\n",
    "  # restore the last checkpoint if there is one\n",
    "  checkpoint.restore(manager.latest_checkpoint)\n",
    "  if manager.latest_checkpoint:\n",
    "    print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "  else:\n",
    "    print(\"Initializing from scratch.\")\n",
    "\n",
    "  train_summary_writer = tf.summary.create_file_writer(log_directory)\n",
    "  valid_summary_writer = tf.summary.create_file_writer(log_directory)\n",
    "\n",
    "  # Initialize W&B run\n",
    "  wandb.init(project='your_project_name', name='your_experiment_name', resume = '0')\n",
    "\n",
    "\n",
    "  # training loop starts\n",
    "  for epoch in range(epochs):\n",
    "    checkpoint.step.assign_add(1)\n",
    "    print(f\"\\n Epoch {checkpoint.step.numpy()} : \\n\")\n",
    "\n",
    "    # initialize losses and metrics\n",
    "    train_total_loss = 0\n",
    "    valid_total_loss = 0\n",
    "    valid_total_metric = 0\n",
    "    train_n_batches = 0\n",
    "    valid_n_batches = 0\n",
    "\n",
    "    # train loop\n",
    "    for step, in enumerate(tqdm(train_dataset, desc='Training', position=0)):\n",
    "      loss_value = train_step(X_batch, y_batch, model, loss_fn, optimizer)\n",
    "\n",
    "      train_total_loss += loss_value\n",
    "      train_n_batches += 1\n",
    "\n",
    "    # valid loop\n",
    "    for step, (X_batch, y_batch) in enumerate(tqdm(valid_dataset, desc='Validation', position=0)):\n",
    "      loss_value, metric_value = valid_step(X_batch, y_batch, model, loss_fn, metric_fn)\n",
    "      valid_total_loss += loss_value\n",
    "      valid_total_metric += metric_value\n",
    "      valid_n_batches += 1\n",
    "\n",
    "    train_epoch_loss = train_total_loss / train_n_batches\n",
    "    valid_epoch_loss = valid_total_loss / valid_n_batches\n",
    "    valid_epoch_metric = valid_total_metric / valid_n_batches\n",
    "\n",
    "    train_losses.append(train_epoch_loss)\n",
    "    valid_losses.append(valid_epoch_loss)\n",
    "    valid_metrics.append(valid_epoch_metric)\n",
    "\n",
    "    # Log training metrics for the epoch\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('train_loss', train_epoch_loss, step=checkpoint.step.numpy())\n",
    "\n",
    "    # Log validation metrics for the epoch\n",
    "    with valid_summary_writer.as_default():\n",
    "        tf.summary.scalar('valid_loss', valid_epoch_loss, step=checkpoint.step.numpy())\n",
    "        tf.summary.scalar('valid_metric', valid_epoch_metric, step=checkpoint.step.numpy())\n",
    "\n",
    "    # Log metrics to W&B\n",
    "    wandb.log({\n",
    "              'train_loss': train_epoch_loss.numpy(),\n",
    "              'valid_loss': valid_epoch_loss.numpy(),\n",
    "              'valid_metric': valid_epoch_metric.numpy(),\n",
    "              'epoch': checkpoint.step.numpy()\n",
    "          })\n",
    "    #save checkpoint\n",
    "    save_path = manager.save()\n",
    "\n",
    "    # Save checkpoint to W&B as an artifact\n",
    "    artifact = wandb.Artifact(f'epoch-{checkpoint.step.numpy()}-checkpoint', type='model')\n",
    "    # Add the index file\n",
    "    artifact.add_file(save_path + \".index\")\n",
    "\n",
    "    # Add the data file\n",
    "    artifact.add_file(save_path + \".data-00000-of-00001\")\n",
    "\n",
    "    wandb.log_artifact(artifact)\n",
    "    end_time = timer()\n",
    "\n",
    "    # print out results\n",
    "    print(\"\\n\")\n",
    "    print(\"  (Results)\")\n",
    "    print(f\"  train_loss: {train_epoch_loss:.4f}\")\n",
    "    print(f\"  valid_loss: {valid_epoch_loss:.4f}\")\n",
    "    print(f\"  valid_metric: {valid_epoch_metric:.4f}\")\n",
    "    print(\"\\n\")\n",
    "    print(\"  (Time)\")\n",
    "    print(\"  start_time : \", start_time)\n",
    "    print(\"  end_time : \", end_time)\n",
    "    print(\"-----------------------------------------------------------------\")\n",
    "\n",
    "  # Finish the W&B run\n",
    "  wandb.finish()\n",
    "\n",
    "  return train_losses, valid_losses, valid_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.binary_classifier()\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(binary_classifier_config.initial_learning_rate,\n",
    "                                                        decay_steps=4000,\n",
    "                                                        alpha=0.0001)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = lr_schedule)\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "metric_fn = tf.keras.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_directory = \"/content/drive/MyDrive/kaggle/contrail_detector/checkpoint/binary_classifier\"\n",
    "log_directory = \"/content/drive/MyDrive/kaggle/contrail_detector/log/binary_classifier\"\n",
    "train_losses, valid_losses, valid_metrics = train_model(train_dataset, valid_dataset, model,\n",
    "                                                        loss_fn, metric_fn, optimizer,\n",
    "                                                        epochs=Config.n_epochs,\n",
    "                                                        checkpoint_directory = checkpoint_directory,\n",
    "                                                        log_directory = log_directory)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
